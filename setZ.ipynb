{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTf201ayaA5C"
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#####################################################################\n",
    "#                           Set Z                                   #\n",
    "#####################################################################\n",
    "# Classic Multi-label algorithms + Neural Networks and Embeddings   #\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "pd.set_option('max_colwidth',400)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, CuDNNLSTM, Bidirectional, Dense, \\\n",
    "    LSTM, Conv1D, MaxPooling1D, Dropout, concatenate, Flatten, add, Conv2D\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import Input, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from preprocess import Preproccesor\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, make_scorer, fbeta_score, multilabel_confusion_matrix,\\\n",
    "                            average_precision_score, precision_score, recall_score\n",
    "import nltk\n",
    "import warnings\n",
    "\n",
    "def average_precision_wrapper(y, y_pred, view):\n",
    "    return average_precision_score(y, y_pred.toarray(),average=view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6038,
     "status": "ok",
     "timestamp": 1577465669993,
     "user": {
      "displayName": "John Seventh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC1p5iNR1xhsyIIDrAtad7L7bm-t8Mpf_W83OJVqw=s64",
      "userId": "04185207313928483885"
     },
     "user_tz": -120
    },
    "id": "yK9HZQFU_JUl",
    "outputId": "535acfa7-4e9e-459f-ec47-0539b51c3097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.6/dist-packages (0.1.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.3.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.17.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install iterative-stratification\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_ljgIk5weaI"
   },
   "outputs": [],
   "source": [
    "hamm_scorer = make_scorer(hamming_loss, greater_is_better=False)\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1837,
     "status": "ok",
     "timestamp": 1577465671856,
     "user": {
      "displayName": "John Seventh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC1p5iNR1xhsyIIDrAtad7L7bm-t8Mpf_W83OJVqw=s64",
      "userId": "04185207313928483885"
     },
     "user_tz": -120
    },
    "id": "__fqdv2uwVkS",
    "outputId": "e7f634f2-bd22-4f71-d402-64a90dbafe1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "X, yt, y = Preproccesor.load_multi_label_data(True) #yt has continuous data, y has binary\n",
    "label_names = [\"isHate\",\"isViolence\",\"isNotViolence\",\"isGeneralized\",\"isDirected\",\"gender\",\"race\",\"national_origin\",\"disability\",\"religion\",\"sexual_orientation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1107521,
     "status": "ok",
     "timestamp": 1577459304965,
     "user": {
      "displayName": "John Seventh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC1p5iNR1xhsyIIDrAtad7L7bm-t8Mpf_W83OJVqw=s64",
      "userId": "04185207313928483885"
     },
     "user_tz": -120
    },
    "id": "n0T1bzq3aD56",
    "outputId": "4f7b80d0-c7f3-4262-c53f-804e7bb40dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-27 14:50:01--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:16a6, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1523785255 (1.4G) [application/zip]\n",
      "Saving to: ‘crawl-300d-2M.vec.zip’\n",
      "\n",
      "crawl-300d-2M.vec.z 100%[===================>]   1.42G  61.6MB/s    in 24s     \n",
      "\n",
      "2019-12-27 14:50:25 (60.1 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
      "\n",
      "--2019-12-27 14:50:27--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
      "--2019-12-27 14:50:28--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
      "--2019-12-27 14:50:28--  http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1877800501 (1.7G) [application/zip]\n",
      "Saving to: ‘glove.42B.300d.zip’\n",
      "\n",
      "glove.42B.300d.zip  100%[===================>]   1.75G  1.87MB/s    in 14m 32s \n",
      "\n",
      "2019-12-27 15:05:00 (2.05 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
      "\n",
      "[<ZipInfo filename='crawl-300d-2M.vec' compress_type=deflate filemode='-rw-r--r--' file_size=4514687127 compress_size=1523784963>]\n",
      "[<ZipInfo filename='glove.42B.300d.txt' compress_type=deflate filemode='-rw-rw-r--' file_size=5025028820 compress_size=1877800207>]\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip'\n",
    "!wget 'http://nlp.stanford.edu/data/glove.42B.300d.zip' \n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"/content/crawl-300d-2M.vec.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "    print(zip_ref.filelist)\n",
    "with zipfile.ZipFile(\"/content/glove.42B.300d.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "    print(zip_ref.filelist)\n",
    "\n",
    "del zip_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6492,
     "status": "ok",
     "timestamp": 1577465676781,
     "user": {
      "displayName": "John Seventh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC1p5iNR1xhsyIIDrAtad7L7bm-t8Mpf_W83OJVqw=s64",
      "userId": "04185207313928483885"
     },
     "user_tz": -120
    },
    "id": "fpIelS9AaFdD",
    "outputId": "d13a6dc4-7927-45e0-b735-7e266079c7eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/content/crawl-300d-2M.vec.zip': No such file or directory\n",
      "rm: cannot remove '/content/glove.42B.300d.zip': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm '/content/crawl-300d-2M.vec.zip'\n",
    "!rm '/content/glove.42B.300d.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6gmfH8eaGt6"
   },
   "outputs": [],
   "source": [
    "embedding_path1 = \"/content/crawl-300d-2M.vec\" #FastText\n",
    "embedding_path2 = \"/content/glove.42B.300d.txt\" #Glove 300d\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXXdvNTlaIH9"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "def build_matrix(embedding_path, tk, max_features):\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding = \"utf-8\"))\n",
    "\n",
    "    word_index = tk.word_index\n",
    "    nb_words = max_features\n",
    "    embedding_matrix = np.zeros((nb_words + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "def create_embedding_matrix(embed, tk, max_features):\n",
    "    if embed == 1:\n",
    "      return build_matrix(embedding_path1, tk, max_features)\n",
    "    elif embed == 2:\n",
    "      return build_matrix(embedding_path2, tk, max_features)\n",
    "    else:\n",
    "      return np.concatenate([build_matrix(embedding_path1, tk, max_features), build_matrix(embedding_path2, tk, max_features)], axis=-1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Ejp2HjdaLov"
   },
   "outputs": [],
   "source": [
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjfmfjD6aNBV"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                              K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGFFWC_GkJHK"
   },
   "outputs": [],
   "source": [
    "def my_hamming_loss(y_true, y_pred):\n",
    "    print(y_true,y_pred)\n",
    "    y_true=K.cast(y_true, dtype='float32')\n",
    "    y_pred=K.cast(y_pred, dtype='float32')\n",
    "    print(y_true,y_pred)\n",
    "    hamming_loss(y_true,y_pred)\n",
    "    \n",
    "    return K.mean(diff, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZs3nWFgaQ8W"
   },
   "outputs": [],
   "source": [
    "#Binary Relevance\n",
    "def build_model1(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "    main_input = Input(shape=(max_len,), name='main_input')\n",
    "    x = (Embedding(max_features + 1, embed_size*2, input_length=max_len, weights=[embedding_matrix], trainable=False))(main_input)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(150, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(150, return_sequences=True))(x)\n",
    "    hidden = concatenate([\n",
    "        Attention(max_len)(x),\n",
    "        GlobalMaxPooling1D()(x),\n",
    "    ])\n",
    "    hidden = Dense(1024, activation='selu')(hidden)\n",
    "    hidden = Dropout(0.3)(hidden)\n",
    "    hidden = Dense(512, activation='selu')(hidden)\n",
    "    hidden = Dropout(0.2)(hidden)\n",
    "    hidden1 = Dense(128, activation='selu')(hidden)\n",
    "    output_lay1 = Dense(11, activation='sigmoid')(hidden1)\n",
    "    model = Model(inputs=[main_input], outputs=output_lay1)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=['binary_accuracy'])\n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file='model1.png')\n",
    "    model2 = Model(inputs=[main_input], outputs=output_lay1)\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=20, validation_data=(X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
    "    model2.load_weights(file_path)\n",
    "    model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=['binary_accuracy'])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oreG7FWTRa3a"
   },
   "outputs": [],
   "source": [
    "#Classifier Chains\n",
    "def build_model2(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "    main_input = Input(shape=(max_len,), name='main_input')\n",
    "    x = (Embedding(max_features + 1, embed_size*2, input_length=max_len, weights=[embedding_matrix], trainable=False))(main_input)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(150, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(150, return_sequences=True))(x)\n",
    "    hidden = concatenate([\n",
    "        Attention(max_len)(x),\n",
    "        GlobalMaxPooling1D()(x),\n",
    "    ])\n",
    "    hidden = Dense(1024, activation='selu')(hidden)\n",
    "    hidden = Dropout(0.3)(hidden)\n",
    "    hidden = Dense(512, activation='selu')(hidden)\n",
    "    hidden = Dropout(0.3)(hidden)\n",
    "    hidden1 = Dense(128, activation='selu')(hidden)\n",
    "    output_lay1 = Dense(1, activation='selu')(hidden1)\n",
    "    hidden2 = concatenate([hidden1,output_lay1])\n",
    "    output_lay2 = Dense(1, activation='selu')(hidden2)\n",
    "    hidden3 = concatenate([hidden2,output_lay2])\n",
    "    output_lay3 = Dense(1, activation='selu')(hidden3)\n",
    "    hidden4 = concatenate([hidden3,output_lay3])\n",
    "    output_lay4 = Dense(1, activation='selu')(hidden4)\n",
    "    hidden5 = concatenate([hidden4,output_lay4])\n",
    "    output_lay5 = Dense(1, activation='selu')(hidden5)\n",
    "    hidden6 = concatenate([hidden5,output_lay5])\n",
    "    output_lay6 = Dense(1, activation='selu')(hidden6)\n",
    "    hidden7 = concatenate([hidden6,output_lay6])\n",
    "    output_lay7 = Dense(1, activation='selu')(hidden7)\n",
    "    hidden8 = concatenate([hidden7,output_lay7])\n",
    "    output_lay8 = Dense(1, activation='selu')(hidden8)\n",
    "    hidden9 = concatenate([hidden8,output_lay8])\n",
    "    output_lay9 = Dense(1, activation='selu')(hidden9)\n",
    "    hidden10 = concatenate([hidden9,output_lay9])\n",
    "    output_lay10 = Dense(1, activation='selu')(hidden10)\n",
    "    hidden11 = concatenate([hidden10,output_lay10])\n",
    "    output_lay11 = Dense(1, activation='selu')(hidden11)\n",
    "\n",
    "    hidden_l = concatenate([output_lay1,output_lay2,output_lay3,output_lay4,output_lay5,output_lay6,\n",
    "                                  output_lay7,output_lay8,output_lay9,output_lay10,output_lay11])\n",
    "    output_layer = Dense(11, activation='sigmoid')(hidden_l)\n",
    "\n",
    "    model = Model(inputs=[main_input], outputs=output_layer)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=['binary_accuracy','categorical_accuracy'])\n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file='model2.png')\n",
    "    model2 = Model(inputs=[main_input], outputs=output_layer)\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
    "    model2.load_weights(file_path)\n",
    "    model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=['binary_accuracy','categorical_accuracy'])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vhpUiAeM_Nf"
   },
   "outputs": [],
   "source": [
    "def build_model3(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "    main_input = Input(shape=(max_len,), name='main_input')\n",
    "    x = (Embedding(max_features + 1, embed_size*2, input_length=max_len, weights=[embedding_matrix], trainable=False))(main_input)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(150, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(150, return_sequences=True))(x)\n",
    "    hidden = concatenate([\n",
    "        Attention(max_len)(x),\n",
    "        GlobalMaxPooling1D()(x),\n",
    "    ])\n",
    "    hidden = Dense(1024, activation='selu')(hidden)\n",
    "    hidden = Dropout(0.3)(hidden)\n",
    "    hidden = Dense(512, activation='selu')(hidden)\n",
    "    hidden = Dropout(0.2)(hidden)\n",
    "    hidden1 = Dense(128, activation='selu')(hidden)\n",
    "    hidden_output_lay1 = Dense(1, activation='relu')(hidden1)\n",
    "\n",
    "    hidden_conc_1 = concatenate([hidden_output_lay1,hidden1])\n",
    "\n",
    "    hidden_output_lay2 = Dense(2, activation='relu')(hidden_conc_1)\n",
    "    hidden_output_lay3 = Dense(2, activation='relu')(hidden_conc_1)\n",
    "    \n",
    "    hidden_output_lay4 = Dense(1, activation='relu')(hidden_conc_1)\n",
    "    hidden_output_lay5 = Dense(1, activation='relu')(hidden_conc_1)\n",
    "    hidden_output_lay6 = Dense(1, activation='relu')(hidden_conc_1)\n",
    "    hidden_output_lay7 = Dense(1, activation='relu')(hidden_conc_1)\n",
    "    hidden_output_lay8 = Dense(1, activation='relu')(hidden_conc_1)\n",
    "    hidden_output_lay9 = Dense(1, activation='relu')(hidden_conc_1)\n",
    "\n",
    "    final_hidden_conc = concatenate([hidden_output_lay1,hidden_output_lay2,hidden_output_lay3,hidden_output_lay4,\n",
    "                                     hidden_output_lay5,hidden_output_lay6,hidden_output_lay7,hidden_output_lay8,\n",
    "                                     hidden_output_lay9])\n",
    "    final_output = Dense(11, activation='sigmoid')(final_hidden_conc)\n",
    "\n",
    "    model = Model(inputs=[main_input], outputs=final_output)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=['binary_accuracy'])\n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file='model3.png')\n",
    "    model2 = Model(inputs=[main_input], outputs=final_output)\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
    "    model2.load_weights(file_path)\n",
    "    model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=['binary_accuracy'])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTkSEEkfaSYS"
   },
   "outputs": [],
   "source": [
    "max_features = 50000\n",
    "scores = {}\n",
    "scores = {}\n",
    "scores.setdefault('test_F1_example', [])\n",
    "scores.setdefault('test_F1_macro', [])\n",
    "scores.setdefault('test_F1_micro', [])\n",
    "scores.setdefault('test_precision_example', [])\n",
    "scores.setdefault('test_precision_macro', [])\n",
    "scores.setdefault('test_precision_micro', [])\n",
    "scores.setdefault('test_recall_example', [])\n",
    "scores.setdefault('test_recall_macro', [])\n",
    "scores.setdefault('test_recall_micro', [])\n",
    "scores.setdefault('test_average_precision_macro', [])\n",
    "scores.setdefault('test_average_precision_micro', [])\n",
    "scores.setdefault('test_Accuracy', [])\n",
    "scores.setdefault('test_Hamm', [])\n",
    "cm = []\n",
    "mskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n",
    "fold_n=0\n",
    "save_ys = []\n",
    "save_yt = []\n",
    "max_len = 150\n",
    "embed_size = 150\n",
    "embma = 1\n",
    "name = \"Mixed2\"\n",
    "for train_index, test_index in mskf.split(X, y):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    tk = Tokenizer(lower = True, filters='', num_words=max_features, oov_token = True)\n",
    "    tk.fit_on_texts(X_train)\n",
    "    train_tokenized = tk.texts_to_sequences(X_train)\n",
    "    valid_tokenized = tk.texts_to_sequences(X_valid)\n",
    "    X_train = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "    X_valid = pad_sequences(valid_tokenized, maxlen=max_len)\n",
    "    embedding_matrix = create_embedding_matrix(embma, tk, max_features)\n",
    "\n",
    "    model = build_model4(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=1e-3, lr_d=0, spatial_dr=0.1, dense_units=128, conv_size=128, dr=0.1, patience=10, fold_id=fold_n)\n",
    "      \n",
    "    fold_n = fold_n + 1\n",
    "    yT = model.predict(X_valid)\n",
    "    y_preds = []\n",
    "    for yt in yT: #Don't do this if you throw them with continuous values\n",
    "      yi = []\n",
    "      for i in yt:\n",
    "        if i>=0.5:\n",
    "          yi.append(int(1))\n",
    "        else:\n",
    "          yi.append(int(0))\n",
    "      y_preds.append(yi)\n",
    "    y_preds = np.array(y_preds)\n",
    "    cm.append(multilabel_confusion_matrix(y_valid,y_preds))\n",
    "    scores['test_F1_example'].append(f1_score(y_valid, y_preds, average='samples'))\n",
    "    scores['test_F1_macro'].append(f1_score(y_valid, y_preds, average='macro'))\n",
    "    scores['test_F1_micro'].append(f1_score(y_valid, y_preds, average='micro'))\n",
    "    scores['test_precision_example'].append(precision_score(y_valid, y_preds, average='samples'))\n",
    "    scores['test_precision_macro'].append(precision_score(y_valid, y_preds, average='macro'))\n",
    "    scores['test_precision_micro'].append(precision_score(y_valid, y_preds, average='micro'))\n",
    "    scores['test_recall_example'].append(recall_score(y_valid, y_preds, average='samples'))\n",
    "    scores['test_recall_macro'].append(recall_score(y_valid, y_preds, average='macro'))\n",
    "    scores['test_recall_micro'].append(recall_score(y_valid, y_preds, average='micro'))\n",
    "    scores['test_average_precision_macro'].append(average_precision_score(y_valid, y_preds, average='macro'))\n",
    "    scores['test_average_precision_micro'].append(average_precision_score(y_valid, y_preds, average='micro'))\n",
    "    scores['test_Accuracy'].append(accuracy_score(y_valid, y_preds))\n",
    "    scores['test_Hamm'].append(hamming_loss(y_valid, y_preds))\n",
    "cmt = cm[0]\n",
    "for ra in range(1,len(cm)):\n",
    "    cmt = cmt + ra\n",
    "cmt = cmt/10\n",
    "print(cmt)\n",
    "f = open(\"setE.txt\", \"a+\")\n",
    "f.write(\"{:<7} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} \\n\".format(str(name)[:7],\n",
    "                                              str('%.4f' % (sum(scores['test_F1_example'])/10)),\n",
    "                                              str('%.4f' % (sum(scores['test_F1_macro'])/10)),\n",
    "                                              str('%.4f' % (sum(scores['test_F1_micro']) / 10)),\n",
    "                                              str('%.4f' % (sum(scores['test_precision_example']) / 10)),\n",
    "                                              str('%.4f' % (sum(scores['test_precision_macro']) / 10)),\n",
    "                                              str('%.4f' % (sum(scores['test_precision_micro']) / 10)),\n",
    "                                              str('%.4f' % (sum(scores['test_recall_example']) / 10)),\n",
    "                                              str('%.4f' % (sum(scores['test_recall_macro']) / 10)),\n",
    "                                              str('%.4f' % (sum(scores['test_recall_micro']) / 10)),\n",
    "                                              str('%.4f' % (sum(scores['test_average_precision_macro'])/10)),\n",
    "                                              str('%.4f' % (sum(scores['test_average_precision_micro'])/10)),\n",
    "                                              str('%.4f' % (sum(scores['test_Accuracy'])/10)),\n",
    "                                              str('%.4f' % (sum(scores['test_Hamm'])/10))))\n",
    "f.close()\n",
    "print(\"{:<7} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} \\n\".format(str(name)[:7],\n",
    "                                            str('%.4f' % (sum(scores['test_F1_example'])/10)),\n",
    "                                            str('%.4f' % (sum(scores['test_F1_macro'])/10)),\n",
    "                                            str('%.4f' % (sum(scores['test_F1_micro']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_precision_example']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_precision_macro']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_precision_micro']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_recall_example']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_recall_macro']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_recall_micro']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_average_precision_macro'])/10)),\n",
    "                                            str('%.4f' % (sum(scores['test_average_precision_micro'])/10)),\n",
    "                                            str('%.4f' % (sum(scores['test_Accuracy'])/10)),\n",
    "                                            str('%.4f' % (sum(scores['test_Hamm'])/10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2198599,
     "status": "ok",
     "timestamp": 1577462798972,
     "user": {
      "displayName": "John Seventh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC1p5iNR1xhsyIIDrAtad7L7bm-t8Mpf_W83OJVqw=s64",
      "userId": "04185207313928483885"
     },
     "user_tz": -120
    },
    "id": "O2Gyj_mMYhDR",
    "outputId": "81dcbca5-bff2-4d8a-f467-033a945a7a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed   | 0.1983  0.2398  0.4633  0.2783  0.3182  0.6211  0.1701  0.2220  0.3803  0.2344  0.3283  0.4248  0.1288  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"{:<7} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} \\n\".format(str(name)[:7],\n",
    "                                            str('%.4f' % (sum(scores['test_F1_example'])/10)),\n",
    "                                            str('%.4f' % (sum(scores['test_F1_macro'])/10)),\n",
    "                                            str('%.4f' % (sum(scores['test_F1_micro']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_precision_example']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_precision_macro']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_precision_micro']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_recall_example']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_recall_macro']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_recall_micro']) / 10)),\n",
    "                                            str('%.4f' % (sum(scores['test_average_precision_macro'])/10)),\n",
    "                                            str('%.4f' % (sum(scores['test_average_precision_micro'])/10)),\n",
    "                                            str('%.4f' % (sum(scores['test_Accuracy'])/10)),\n",
    "                                            str('%.4f' % (sum(scores['test_Hamm'])/10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXVJ2WBK-o25"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "setZ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
