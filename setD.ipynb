{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hR3FalYEjxU3"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
    "  <tr>   \n",
    "    <th>\n",
    "      <a target=\"_blank\" href=\"https://colab.research.google.com/github.com/iamollas/hateSpeechDataV2/blob/master/setD.ipynb\"> <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"/></a>\n",
    "    </th>\n",
    "    <th>\n",
    "      <a target=\"_blank\" href=\"https://github.com/iamollas/hateSpeechDataV2/blob/master/setD.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"/></a>\n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>\n",
    "      <a target=\"_blank\" href=\"https://colab.research.google.com/github.com/iamollas/hateSpeechDataV2/blob/master/setD.ipynb\">\n",
    "      Run in Google Colab</a>\n",
    "    </th>\n",
    "    <th>\n",
    "      <a target=\"_blank\" href=\"https://github.com/iamollas/hateSpeechDataV2/blob/master/setD.ipynb\">\n",
    "      View source on GitHub</a>\n",
    "    </th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIddtfgDmk6Y"
   },
   "source": [
    "Do you plan to test it on Colab? Make sure you change your settings to: Runtime->Change runtime type->Hardware accelarator->GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snUCpRg7rMAo"
   },
   "source": [
    "We import some necessary libraries and we load our hate speech dataset.\n",
    "The preprocess script is available on the github repo.\n",
    "\n",
    "Check code here: https://towardsdatascience.com/how-to-do-text-binary-classification-with-bert-f1348a25d905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "jyu3Ehl9Hpoi",
    "outputId": "e010f026-4f5b-4288-f279-b97eab05c710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#####################################################################\n",
    "#                           Set D                                   #\n",
    "#####################################################################\n",
    "#                        BERT Tests                                 #\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "pd.set_option('max_colwidth',400)\n",
    "from preprocess import Preproccesor\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf  \n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "X, y = Preproccesor.load_data(True)\n",
    "class_names = ['noHateSpeech', 'hateSpeech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1aNBBGtMrknU"
   },
   "source": [
    "We want to measure specificity and sensitivity, so we manually create those two functions for these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cqoKzC2_faU"
   },
   "outputs": [],
   "source": [
    "def specificity(tn, fp, fn, tp):\n",
    "    tn, fp, fn, tp\n",
    "    if(tn+fp)>0:\n",
    "        speci = tn/(tn+fp)\n",
    "        return speci\n",
    "    return 0\n",
    "def sensitivity(tn, fp, fn, tp):\n",
    "    tn, fp, fn, tp\n",
    "    if(tp+fn)>0:\n",
    "        sensi = tp/(tp+fn)\n",
    "        return sensi\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WEr8_weUr_FU"
   },
   "source": [
    "More over, for our bert experiments we will need the bert-text library in order to have more compact code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 713
    },
    "colab_type": "code",
    "id": "EQHcJlAgHZ2o",
    "outputId": "3d88b5e8-7461-4928-830c-b1eee0db9e5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-text\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/1d/3c7226547cd6a2ba54586dfd80a49f64a0936f9bcb945a4686e843a1956a/bert_text-5.0.0-py3-none-any.whl\n",
      "Collecting tensorflow-gpu (from bert-text)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
      "\u001b[K     |████████████████████████████████| 377.0MB 118kB/s \n",
      "\u001b[?25hCollecting bert-tensorflow (from bert-text)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from bert-text) (0.24.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-text) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (from bert-text) (0.6.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bert-text) (4.28.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (0.1.7)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (0.33.6)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (1.0.8)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (1.11.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (0.8.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (0.2.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (3.7.1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (1.14.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (1.16.5)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (1.14.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (0.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu->bert-text) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->bert-text) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->bert-text) (2.5.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu->bert-text) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu->bert-text) (41.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu->bert-text) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu->bert-text) (0.15.6)\n",
      "Installing collected packages: tensorflow-gpu, bert-tensorflow, bert-text\n",
      "Successfully installed bert-tensorflow-1.0.1 bert-text-5.0.0 tensorflow-gpu-1.14.0\n"
     ]
    },
    {
     "data": {},
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install bert-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJ7qG9yOsBHQ"
   },
   "source": [
    "Now we our testing with a 10-Fold Cross Validation process the BERT's performance. In every loop we load BERT and we perform fine tuning using the new train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJZwZjvYc0Dx"
   },
   "outputs": [],
   "source": [
    "  !rm '/content/test.tsv'\n",
    "  !rm '/content/train.tsv'\n",
    "  !rm -R '/content/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "EZPKf9PaHgcW",
    "outputId": "79fb8ec0-381b-4cf1-d377-4c7fd567430b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Thu Sep 12 16:50:09 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'auc': 0.7444179, 'eval_accuracy': 0.75247526, 'f1_score': 0.70588225, 'false_negatives': 14.0, 'false_positives': 11.0, 'loss': 1.3915498, 'precision': 0.73170733, 'recall': 0.6818182, 'true_negatives': 46.0, 'true_positives': 30.0, 'global_step': 560}\n",
      "Fold 1 started at Thu Sep 12 16:59:12 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "f_results = []\n",
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=7)\n",
    "counter = 0\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "  from bert_text import run_on_dfs\n",
    "  print('Fold', fold_n, 'started at', time.ctime())\n",
    "  X_train, X_test = X[train_index], X[valid_index]\n",
    "  y_train, y_test = y[train_index], y[valid_index]\n",
    "  ids = []\n",
    "  idsT = []\n",
    "  for i in range(0,len(X_train)):\n",
    "    ids.append(i)\n",
    "  for i in range(0,len(X_test)):\n",
    "    idsT.append(i)\n",
    "  train = pd.DataFrame(X_train, columns=['comment_text'],index=ids)\n",
    "  train['target'] = y_train\n",
    "  test = pd.DataFrame(X_test, columns=['comment_text'],index=idsT)\n",
    "  test['target'] = y_test\n",
    "  train.to_csv('train.tsv', sep='\\t', index=False, header=False)\n",
    "  test.to_csv('test.tsv', sep='\\t', index=False, header=True)\n",
    "  myparam = {\n",
    "    \"DATA_COLUMN\": \"comment_text\",\n",
    "    \"LABEL_COLUMN\": \"target\",\n",
    "    \"MAX_SEQ_LENGTH\":100,\n",
    "    \"BATCH_SIZE\":16,\n",
    "    \"LEARNING_RATE\": 2e-5,\n",
    "    \"NUM_TRAIN_EPOCHS\": 10,\n",
    "    \"SAVE_SUMMARY_STEPS\":100,#100\n",
    "    \"SAVE_CHECKPOINTS_STEPS\":10000#10000\n",
    "  }\n",
    "  result, estimator = run_on_dfs(train, test, **myparam)\n",
    "  f_results.append(result)\n",
    "  print(len(f_results),result)\n",
    "  del result, estimator\n",
    "  !rm '/content/test.tsv'\n",
    "  !rm '/content/train.tsv'\n",
    "  !rm -R '/content/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ApYbEwoVvFIR"
   },
   "source": [
    "Finally, we compute the metrics we want and we save them to a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69vwNde1c-2q"
   },
   "outputs": [],
   "source": [
    "acc = 0\n",
    "f1 = 0\n",
    "sensi = 0 \n",
    "speci = 0\n",
    "preci = 0\n",
    "recall = 0\n",
    "for i in f_results:\n",
    "  tn = i['true_negatives']\n",
    "  fp = i['false_positives']\n",
    "  fn = i['false_negatives']\n",
    "  tp = i['true_positives']\n",
    "  acc = acc + i['eval_accuracy']\n",
    "  f1 = f1 + i['f1_score']\n",
    "  preci = preci + i['precision']\n",
    "  recall = recall + i['recall']\n",
    "  speci = speci + specificity(tn, fp, fn, tp)\n",
    "  sensi = sensi + sensitivity(tn, fp, fn, tp)\n",
    "print(\"{:<7} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7}\".format('Method','F1score','Precisi','Recall','Accurac','Specifi','Sensiti'))\n",
    "print(\"=========================================================================\")\n",
    "print(\"{:<7} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7}\".format(str(\"BERT\")[:7], str('%.4f' % (f1/10)),str('%.4f' % (preci/10)), \n",
    "                                                           str('%.4f' % (recall/10)), str('%.4f' % (acc/10)), str('%.4f' % (speci/10)), \n",
    "                                                           str('%.4f' % (sensi/10))))\n",
    "f = open(\"setD.txt\", \"a+\")\n",
    "f.write(\"{:<7} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7}\".format('Method','F1score','Precisi','Recall','Accurac','Specifi','Sensiti'))\n",
    "f.write(\"=========================================================================\\n\")\n",
    "f.write(\"{:<7} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7}\".format(str(\"BERT\")[:7], str('%.4f' % (f1/10)), str('%.4f' % (preci/10)), \n",
    "                                                             str('%.4f' % (recall/10)), str('%.4f' % (acc/10)), str('%.4f' % (speci/10)), \n",
    "                                                             str('%.4f' % (sensi/10))))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BertFinally.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
